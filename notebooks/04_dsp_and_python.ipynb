{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Dive: DSP & Python\n",
    "\n",
    "![](assets/transitions/lab_trans_dsp_python.svg)\n",
    "\n",
    "> In this notebook we'll introduce some development tools for digital signal processing (DSP) using Python and JupyterLab. Unlike the previous notebooks, there is absolutely no hidden code here. We're showing you the complete development story behind an example DSP application.\n",
    ">\n",
    "> In our example application, we'll start by visualising some interesting signals — audio recordings of Scottish birds! We'll then use a few different analytical techniques to gain some understanding of these signals and finally process the audio to isolate a single type of bird.\n",
    "\n",
    "## Inspecting our signal\n",
    "\n",
    "In the assets folder there is an audio file, `birds.wav`. This was recorded by Stuart Fisher and released under [CC BY-NC-ND 2.5](https://creativecommons.org/licenses/by-nc-nd/2.5/); accessible [here](https://www.xeno-canto.org/28039).\n",
    "\n",
    "Before we get into our signal processing at all, let's give it a listen. We can do that through our browser using IPython's rich set of display functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(\"assets/pdd/birds.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so what are we hearing? We've got two main subjects here:\n",
    "  1. The lower pitched bird (going \"cuurloo!\") is a eurasian curlew\n",
    "  2. The higher pitched bird chatting away is a chaffinch\n",
    "\n",
    "Just for context, here's what these birds look like:\n",
    "<div style='max-width: 1005px;'>\n",
    "<div style='width:45%; float:left; text-align:center;'>\n",
    "    <img src=\"assets/pdd/curlew.jpg\"/>\n",
    "    <b>Curlew</b> <br/>Photo by Vedant Raju Kasambe <br/> <a href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.en\">Creative Commons Attribution-Share Alike 4.0</a>\n",
    "</div>\n",
    "<div style='width:45%; float:right; text-align:center;'>\n",
    "    <img src=\"assets/pdd/chaffinch.jpg\"/>\n",
    "    <b>Chaffinch</b> <br/>Photo by Charles J Sharp <br/> <a href=\"https://creativecommons.org/licenses/by/3.0/deed.en\">Creative Commons Attribution 3.0</a>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading from disk\n",
    "\n",
    "Let's get this audio file loaded in Python so we can perform some visualisation. We're going to make use of the [SciPy](https://www.scipy.org/) ecosystem for most of our signal processing in Python. To load the `.wav` file in to our environment as an array of samples, let's use SciPy's `wavfile` IO module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "fs, aud_in = wavfile.read(\"assets/pdd/birds.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`wavfile.read` gives us two things: the sampling frequency of the signal (`fs`), and the raw samples as an array (`aud_in`). Let's check the sampling frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling frequency of the recording is 44.1 kHz — the standard rate for CD quality audio. Now let's look at the format of the samples themselves. To start, what is the type of our sample array? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(aud_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an N-dimensional array ('ndarray') from the NumPy package, that you'll remember from the introduction notebook.\n",
    "\n",
    "Let's interrogate this array a little futher. We should be aware of its length and the data type of each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aud_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_in.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each sample is a signed 16 bit integer, and we have over half a million samples in total! We can comfortably fit this in memory (it's just over 1 MB) but we will need to do some processing to visualise all of this data in a useful format.\n",
    "\n",
    "### Plotting in the time domain\n",
    "\n",
    "As a first investigation, let's plot only a short clip from the recording. We'll use [plotly_express](https://www.plotly.express/) here because it generates impressive, interactive plots with surprisingly small amounts of code. `plotly_express` expects input data to be given as a [pandas data frame](http://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html#overview), so we'll need to do a little bit of conversion work upfront. We build up a frame with multiple columns (time and amplitude, in this case) and then we can efficiently traverse, sort, and search the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def to_time_dataframe(samples, fs):\n",
    "    \"\"\"Create a pandas dataframe from an ndarray of 16-bit time domain samples\"\"\"\n",
    "    num_samples = len(samples)\n",
    "    sample_times = np.linspace(0, num_samples/fs, num_samples)\n",
    "    normalised_samples = samples / 2**15\n",
    "    return pd.DataFrame(dict(\n",
    "        amplitude = normalised_samples,  \n",
    "        time      = sample_times\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can turn our sample array into a data frame, let's pass it to `plotly_express` to create a simple, time-domain plot. First let's make a base template for our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive a custom plotting template from `plotly`\n",
    "import plotly.io as pio\n",
    "new_template = pio.templates['plotly']\n",
    "new_template.update(dict(layout = dict(\n",
    "        width         = 800,\n",
    "        autosize      = False,\n",
    "        legend        = dict(x=1.1),\n",
    ")))\n",
    "\n",
    "# Register new template as the default\n",
    "pio.templates['dsp_plot'] = new_template\n",
    "pio.templates.default = 'dsp_plot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get plotly to plot a snippet of the audio, and it will be in the theme we described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly_express as px\n",
    "\n",
    "# Let's take a small subset of the recording\n",
    "aud_clip = to_time_dataframe(aud_in, fs).query('0.3 < time < 0.718')\n",
    "\n",
    "# Plot signal\n",
    "px.line(                                                             # Make a line plot with...\n",
    "    aud_clip,                                                        #   Data frame\n",
    "    x='time', y='amplitude',                                         #   Axes field names\n",
    "    labels = dict(amplitude='Normalised Amplitude', time='Time (s)') #   Axes label names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is interactive. Feel free to zoom in (click and drag) and pan around. You should be able to zoom in far enough to see the single sinusoidal cycles. Double click anywhere on the plot to zoom back out.\n",
    "\n",
    "There is clearly some activity in this waveform, but it's hard to imagine what this should sound like from the time domain alone. Sure we can get a feel for the volume of the signal over time, but what are the different pitches/frequencies in this sound? Let's take a look at the same snippet in the frequency domain to find out.\n",
    "\n",
    "### Plotting in the frequency domain\n",
    "\n",
    "We can use SciPy to perform a Fast Fourier Transform (FFT) to convert our time domain signal into the frequency domain. The `fft` function performs an FFT for our input. Let's try this out on the small audio clip from above.  Note we will first suppress FutureWarnings from scipy - these warnings are meant for Python package features that will be deprecated in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },  
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "\n",
    "def to_freq_dataframe(samples, fs):\n",
    "    \"\"\"Create a pandas dataframe from an ndarray frequency domain samples\"\"\"\n",
    "    sample_freqs = np.linspace(0, fs, len(samples))\n",
    "    return pd.DataFrame(dict(\n",
    "        amplitude = samples[0:int(len(samples)/2)],  \n",
    "        freq      = sample_freqs[0:int(len(samples)/2)]\n",
    "    ))\n",
    "\n",
    "# Take slice of full input\n",
    "aud_clip_numpy = aud_in[int(0.3*fs): int(0.718*fs)] \n",
    "\n",
    "# Perform FFT\n",
    "NFFT = 2**14 # use a generous length here for maximum resolution\n",
    "aud_clip_fft = np.abs(fft(aud_clip_numpy,NFFT))\n",
    "\n",
    "# Plot FFT\n",
    "px.line(\n",
    "    to_freq_dataframe(aud_clip_fft, fs),\n",
    "    x='freq', y='amplitude',\n",
    "    labels = dict(amplitude='Amplitude', freq='Freq (Hz)')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of features to note in the frequency domain that we had totally missed in the time domain:\n",
    "\n",
    "  1. *What a generous sampling rate!*\n",
    "     \n",
    "     As the original sample rate is 44.1 kHz, the recording is able to represent any frequencies up to 22 kHz. However, there are no significant frequency components above 5 kHz so we could resample this signal to have about $\\frac{1}{3}$ of the data and still retain almost all of the useful information. This should speed up calculations and reduce memory requirements.\n",
    "     \n",
    "  2. *Bird identification!*\n",
    "  \n",
    "     There are two clear and distinct signals: one at $\\approx$ 1.7 kHz and one at $\\approx$ 4 kHz. Go back and see just how difficult this is to identify in the time domain.\n",
    "     \n",
    "     The lower frequency signal is from the curlew and the higher frequency is from the chaffinch. There is also some faint noise under 50 Hz from wind picked up by the microphone. It should be possible to employ some filtering to completely isolate one bird's sound from the other, but we'll get back to this later on in the notebook.\n",
    "     \n",
    "We've been able to glean more of an understanding of the signal's composition by using SciPy to view it the frequency domain. There's one final visualisation tool that we should employ here moving on — the spectrogram!\n",
    "\n",
    "### Plotting as a spectrogram\n",
    "\n",
    "The spectrogram can essentially give us a simultaneous view of both time and frequency by plotting how the FFT of the signal varies with time, with a spectrum of colours to represent signal amplitude.\n",
    "\n",
    "These plots are a little more advanced, so we move away from `plotly_express` and use a lower-level plotly API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from scipy.signal import spectrogram, decimate\n",
    "\n",
    "def plot_spectrogram(samples, fs, decimation_factor=3, max_heat=50, mode='2D'):\n",
    "    \n",
    "    # Optionally decimate input\n",
    "    if decimation_factor>1:\n",
    "        samples_dec = decimate(samples, decimation_factor, zero_phase=True)\n",
    "        fs_dec = int(fs / decimation_factor)\n",
    "    else:\n",
    "        samples_dec = samples\n",
    "        fs_dec = fs\n",
    "\n",
    "    # Calculate spectrogram (an array of FFTs from small windows of our signal)\n",
    "    f_label, t_label, spec_data = spectrogram(\n",
    "        samples_dec, fs=fs_dec, mode=\"magnitude\"\n",
    "    )\n",
    "    \n",
    "    # Make a plotly heatmap/surface graph\n",
    "    layout = go.Layout(\n",
    "        height=500,\n",
    "        # 2D axis titles\n",
    "        xaxis=dict(title='Time (s)'),\n",
    "        yaxis=dict(title='Frequency (Hz)'),\n",
    "        # 3D axis titles\n",
    "        scene=dict(\n",
    "            xaxis=dict(title='Time (s)'),\n",
    "            yaxis=dict(title='Frequency (Hz)'),\n",
    "            zaxis=dict(title='Amplitude')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    trace = go.Heatmap(\n",
    "        z=np.clip(spec_data,0,max_heat),\n",
    "        y=f_label,\n",
    "        x=t_label\n",
    "    ) if mode=='2D' else go.Surface(\n",
    "        z=spec_data,\n",
    "        y=f_label,\n",
    "        x=t_label\n",
    "    )\n",
    "    \n",
    "    py.iplot(dict(data=[trace], layout=layout))\n",
    "\n",
    "\n",
    "plot_spectrogram(aud_in, fs, mode='2D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can see the two bird noises quite distinctly — the curlew between 1.2 $\\rightarrow$ 2.6 kHz and the chaffinch between 3 $\\rightarrow$ 5 kHz. This time, however, we can see how these sounds change over time. The curlew has a smooth sweeping call followed by a short, constant tone while the chaffinch produces a more erratic spectrogram as it jumps between tones in quick succession.\n",
    "\n",
    "Next we'll look at designing some filters from Python so we can isolate one of the birds.\n",
    "\n",
    "## FIR filtering\n",
    "\n",
    "We can use functions from SciPy's signal module to design some FIR filter coefficients and perform the filtering:\n",
    "\n",
    "  * `firwin` can design filter weights that meet a given spec — cut off frequencies, ripple, filter type...\n",
    "  * `freqz` helps us calculate the frequency response of the filter. Useful for checking the characteristics of the generated filter weights.\n",
    "  * `lfilter` actually performs the filtering of our signal.\n",
    "  \n",
    ">If you have used MATLAB these functions will feel familiar to you. One thing to note though is, unlike MATLAB, arrays (or lists) in Python are zero-indexed and array elements are referenced by square brackets, rather than parentheses.\n",
    "\n",
    "### High-pass filter for chaffinch isolation\n",
    "\n",
    "Let's start by designing a filter to isolate the chaffinch sounds. This should be a high-pass filter with the aim of suppressing all signals below 2.6 kHz approximately. To give ourselves some breathing space, we should ask for a filter with a cutoff frequency a little higher than 2.6 kHz; let's say 2.8 kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import freqz, firwin\n",
    "\n",
    "nyq = fs / 2.0\n",
    "taps = 99\n",
    "\n",
    "# Design high-pass filter with cut-off at 2.8 kHz\n",
    "hpf_coeffs = firwin(taps, 2800/nyq, pass_zero=False)\n",
    "\n",
    "def plot_fir_response(coeffs, fs):\n",
    "    \"\"\"Plot the frequency magnitude response of a set of FIR filter weights\"\"\"\n",
    "    \n",
    "    freqs, resp = freqz(coeffs, 1)\n",
    "    return px.line(\n",
    "        to_freq_dataframe(np.abs(resp), fs/2),\n",
    "        x='freq', y='amplitude',\n",
    "        labels = dict(amplitude='Normalised amplitude', freq='Freq (Hz)')\n",
    "    )\n",
    "\n",
    "# We'll also be using these coefficients in the next notebook so let's save them to a file for later...\n",
    "np.save('assets/pdd/hpf_coeffs.npy', hpf_coeffs)\n",
    "\n",
    "# Plot our filter's frequency response as a sanity check\n",
    "plot_fir_response(hpf_coeffs, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we asked for a cut-off frequency of 2.8 kHz and we can use the cursor with the plot above to verify this. Hover over the trace at $\\approx$0.5 amplitude and it should report that this point corresponds to 2.8 kHz.\n",
    "\n",
    "Now it's time to use these filter coefficients to filter the original audio! Let's do this in software with `lfilter` just now, plot the resulting spectrogram, and save a `.wav` file for playback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "\n",
    "# Filter audio\n",
    "aud_hpf = lfilter(hpf_coeffs, 1.0, aud_in)\n",
    "\n",
    "# Plot filtered audio\n",
    "plot_spectrogram(aud_hpf, fs)\n",
    "\n",
    "# Offer audio widget to hear filtered audio\n",
    "wavfile.write('assets/pdd/hpf.wav', fs, np.array(aud_hpf, dtype=np.int16))\n",
    "Audio('assets/pdd/hpf.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully we can confirm both visually and aurally that we've isolated the chaffinch sounds from the curlew and the wind. Sounds pretty good! \n",
    "\n",
    ">It is also possible to isolate the curlew, this time with a bandpass filter. If time permits, design and implement the filter using the techniques we've covered above and plot the results (check out the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.firwin.html) for the `firwin` function if you need help)\n",
    "\n",
    "## Summary \n",
    "\n",
    "We've reached the end of our first *Deeper Dive* notebook, so let's quickly recap what we've covered:\n",
    "\n",
    "  * Using the JupyterLab and Python environment as a DSP prototyping platform:\n",
    "    + Introducing the SciPy ecosystem, including the `scipy.signal` module for DSP operations and `numpy` for efficient arrays.\n",
    "    + Visualisation with `plotly_express` and `pandas` data frames\n",
    "  * Using Python to inspect signals in the time and frequency domains\n",
    "  * Designing FIR filters with SciPy and verifying their frequency responses\n",
    "  * Performing FIR filtering in software\n",
    "      \n",
    "In the next *Deeper Dive* we will use the techniques learned here to interact with DSP IP on the FPGA. Using the power of PYNQ, we will then control this hardware directly from the notebook!\n",
    "\n",
    "[⬅️ Previous](03_pynq_and_sd_fec.ipynb) 👩‍💻  [Next ➡️](05_dsp_and_pynq.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
